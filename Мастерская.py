#!/usr/bin/env python
# coding: utf-8

# # Проект Мастерской "Аналитика HH.ru вакансий"

# В качестве проекта в рамках Мастерской Яндекс.Практикум нам было предложено провести анализ вакансий на сайте HH.ru по следующим специальностям: аналитик данных и Data Scientist. В качестве источника информации нам было предоставлено два датафрейма со списком вакансий на обе специальности. 
# 
# Цель проекта: выявить различия в предлагаемых вакансиях для аналитиков данных и специалистов по Data Science.
# 
# Задачи проекта:
# 
# - определить доли грейдов Junior, Junior+, Middle, Senior среди вакансий Аналитик данных и Специалист по Data Science
# - определить наиболее желаемых кандидатов на акансии Аналитик данных и Специалист по Data Science по следующим параметрам: самые важные hard-skils, самые важные soft-skils отдельно для грейдов Junior, Junior+, Middle, Senior
# - определить типичное место работы для Аналитика данных и специалист по Data Science по следующим параметрам: ТОП-работодателей, зарплата, тип занятости, график работы отдельно для грейдов Junior, Junior+, Middle, Senior
# - расчитать помесячную динамику количества вакансий для Аналитика данных и специалиста по Data Science. Ответ отдельно дайте для грейдов Junior, Junior+, Middle, Senior
# - сформулировать выводы и рекомендации

# План работы: 
# 
# - изучить предложенные датафреймы и определить, нужна ли предобратка данных
# - провести предобработку, если она понадобится, добавить новые столбцы, которые помогут решить поставленные перед нами задачи
# - провести анализ данных, решив все поставленные перед нами задачи
# - сформулировать выводы и рекомендации
# 
# Подобный план нужно будет реализовать дважды для двух датафреймов.

# In[1]:


# зугрузим необходимые нам библиотеки
import pandas as pd 
import matplotlib.pyplot as plt 
import seaborn as sns 
import plotly.express as px
from collections import Counter


# ## Анализ таблицы с вакансиями на специальность "Data scientist"

# ## 1. Получение общей информации

# In[2]:


# посмотрим на предложенный датафрейм
data = pd.read_excel('C:\\Users\\prysk\\OneDrive\\Документы\\vacancies_ds.xlsx')
data.head(10)


# In[3]:


# и на общую информацию о нем 
data.info()


# ## Выводы
# 
# Мы ознакомились с датафреймом и начальной информацией по нему. Типы данных везде подходящие. Бросается в глаза большое количество пропусков в столбцах с заработной платой. Посмотрим, что можно будет с этим сделать. Также довольно много пропусков в разделе со скиллами, но не настолько критичное. Думаю, даже с этим количеством отсутствующих значений можно будет сделать определенные выводы. 

# ## 2. Предобработка данных

# ### 2.1 Проверка на дубликаты

# In[4]:


# проверим датафрейм на явные дупликаты 
data.duplicated().sum()


# In[5]:


# и на неявные по id 
data['id'].duplicated().sum()


# Дубликатов нет.

# #### 2.2 Заполнение пропусков

# Для того, чтобы заполнить пропущенные значения в столбце с ЗП, посмотрим на те строки, где ЗП указана. Возможно, есть некоторые взаимосвязи. 

# In[6]:


data[data['salary_from'].isnull() == False]


# Никаких взаимосвязей не прослеживается. Разные работадатели, разный требуемый опыт работы и город. Не всегда, где указан нижний порог ЗП, указан и верхний. Поэтому заполнять пропуски средним значением зп по требуемому опыту будет большим допущением. Мы можем увидеть, что где-то джуну+ предлагают значительно больше денег, чем мидлу. Или как в Новосибирске предлагают сильно больше, чем в Москве на тот же грейд. Если бы пропущенных значений было меньше на это же количество строк, тогда можно было воспользоваться средним, в нашем же случае нам это лишь навредит. 

# #### 2.3 Добавление новых столбцов

# Для дальнейшей работы заполним пропуски в столбцах key_skills_from_key_skills_field, hard_skills_from_description и soft_skills_from_description заглушками и объдиними все значения в один столбец.

# In[7]:


data['hard_skills_from_description'] = data['hard_skills_from_description'].fillna(value = '-')
data['soft_skills_from_description'] = data['soft_skills_from_description'].fillna(value = '-')
data['key_skills_from_key_skills_field'] = data['key_skills_from_key_skills_field'].fillna(value = '-')
data['key_skills_key'] = data['hard_skills_from_description'] + ', ' + data['soft_skills_from_description'] + ', ' + data['key_skills_from_key_skills_field']


# Посмотрим на уникальные значения получившегося столбца. 

# In[8]:


data['key_skills_key'].unique()


# In[9]:


# заменим некоторые значения в столбце, которые означают одно и то же
data['key_skills_key'] = data['key_skills_key'].str.replace('ML', 'Machine Learning')
data['key_skills_key'] = data['key_skills_key'].str.replace('Machine Learning', 'Машинное обучение')
data['key_skills_key'] = data['key_skills_key'].str.replace('Data Analysis', 'Анализ данных')
data['key_skills_key'] = data['key_skills_key'].str.replace('Машинное обучение ', 'Машинное обучение')


# И наконец мы добавим в датафрейм столбец, в котором будет выделен месяц размещения вакансии, чтобы выполнить задачу с помесячной динамикой. 

# In[10]:


data['month'] = data['published_at'].dt.to_period("M")


# Посмотрим, что у нас в итоге вышло.

# In[11]:


data.head(10)


# ### Выводы
# 
# Нами была проделана предобработка данных. Было сделано следующее:
# - проверили таблицу на наличие дубликатов, которых не оказалось
# - из-за недостатка данных по заработной плате, восполнить пропуски не получилось
# - для удобства в дальнейшей работе были добавлены два новых столбца со всеми скиллами и месяцем, когда была размещена вакансии

# ## 3. Анализ данных

# ### 3.1. Подсчет доли грейдов

# In[12]:


# Сгруппируем данные по грейду и подсчитаем, какое количество вакансий прихоодится на каждый грейд
data_exp = data.groupby('experience').agg({'id' : 'count'}).sort_values(by = 'id', ascending = False).reset_index()
data_exp


# In[13]:


# Переименуем столбцы для удобства и добавим столбик с процентным отношением 
data_exp['percent'] = data_exp['id']/data_exp['id'].sum()*100
data_exp['percent'] = data_exp['percent'].round(decimals=1)
data_exp.columns = ['Требуемый опыт работы', 'Количество вакансий', 'Доля от общего числа %']
data_exp


# In[14]:


# Построим таблицу для наглядности
fig = px.bar(data_exp,
            x = 'Требуемый опыт работы',
            y = 'Доля от общего числа %',
            title = 'Доля вакансий по критерию "Требуемый опыт работы"',
            template = 'seaborn')
fig.show()


# ###  Выводы
# Больше всего вакансий на грейд Мидл - около 49% от общего числа вакансий. Следом с небольшим отрывом идет Джун+ - 43.7% от общего числа. Дальше с огромным отрывом идут Джуны без опыта работ и Сеньоры с 3.9 и 3.5% соответственно. 

# ### 3.2. Определение самых важных скиллов для всех грейдов

# #### 3.2.1 Junior

# In[15]:


# Для начала создадим срез с строками, где в разделе "опыт" треубются люди без опыта
data_j = data.query('experience == "Junior (no experince)"')
data_j.head()


# In[16]:


data_j['key_skills_key'].unique()


# Видим, что требования к скиллам потенциального работника сильно разнятся, поэтому наилучшим способом выявления самых важных скиллов будет определение самых частовстречающихся скиллов. Посмотрим, что из этого выйдет. 

# In[17]:


data_js = pd.Series(' , '.join(data_j['key_skills_key']).split(',')).value_counts()[:30]
data_js


# #### Выводы
# 
# Как мы видим, информации не очень много. Одно можно сказать с относительной уверенностью: знание Python вам облегчит поиск работы. Его упоминание мы встречаем 6 раз. Все остальные скиллы редко встречаются и слишком индивидуальны в зависимости от вакансии.

# #### 3.2.2 Junior+

# In[18]:


data_jj = data.query('experience == "Junior+ (1-3 years)"')
data_jjs = pd.Series(' , '.join(data_jj['key_skills_key']).lower().split(',')).value_counts()[:30]
data_jjs


# Нам не удалось до конца избежать некоторых повторов в списке, но они не повлияют на выводы. 

# #### Выводы 
# 
# Совершенно очевидно, что основными хард-скиллами для специалиста Data Science являются умение программировать на Python (встречается около 125 раз), в особенности в библиотеке Pandas (встречается 154 раза), знание SQL (встречается 96 раз), уметь в машинное обучение (70 раз), умение анализировать данные (29 раз). Хорошим подспорьем будет знание юнит-экономики, математической статистики, математического моделирования и статистического анализа. 
# 
# Что касается софт-скиллов, то тут большИй разброс. Часто встречается параметр "Документация" (43 раза). Также важным будет способность в коммуникацию (28 раз) и обладание аналитическим мышлением. Иногда встречается знание английского языка. 

# #### 3.2.3 Middle

# In[19]:


data_m = data.query('experience == "Middle (3-6 years)"')
data_ms = pd.Series(' , '.join(data_m['key_skills_key']).lower().split(',')).value_counts()[:30]
data_ms


# #### Выводы
# 
# Как мы можем видить, требования к мидлу не сильно отличаются от требования к джуну+. Все еще на первом месте Python (141 раз), в частности библиотека Pandas (121 раз). Возрастает потребность в навыках машинного обучения. Учитывая задвоение, оно прописано 107 раз. И чуть снижается потребность в знании SQL (75 раз). Дальше идут те же юнит_экономика и математическая статистика. 
# 
# С софт-скиллами та же ситуация. На первых местах документация (60 раз), коммуникация (41 раз) и аналитическое мышление. 

# #### 3.2.4 Senior (6+ years)	

# In[20]:


data_s = data.query('experience == "Senior (6+ years)"')
data_ss = pd.Series(' , '.join(data_s['key_skills_key']).lower().split(',')).value_counts()[:30]
data_ss


# #### Выводы 
# 
# Несмотря на крайне скудную выборку, мы можем заметить, что и у Сеньоров требования те же самые. Python, Pandas, машинное обучение, SQL, анализ данных. В софт_скиллах та же документация и коммуникация. 

# ### 3.3 Определение дополнительных параметров для всех грейдов (Топ-работодателей, зарплата, тип занятости, график работы)

# #### 3.3.1 Junior 

# ##### 3.3.1.1 Топ-работодателей

# In[21]:


data_je = data_j.groupby('employer').agg({'id':'count'}).sort_values(by = 'id', ascending = False).reset_index()
data_je.columns = ['Работодатель', 'Количество вакансий']
data_je


# In[22]:


fig = px.bar(data_je,
            x = 'Работодатель',
            y = 'Количество вакансий',
            title = 'Количество вакансий у различных работадателей',
            template = 'seaborn')
fig.show()


# ##### Выводы 
# Как мы видим, наибольшее количество вакансий для людей без опыта работы предоставлено Сбером, целых 9. Дальше идет Озон с тремя вакансиями, Ламода и Правительство Москвы с двумя, остальные по одной.

# ##### 3.3.1.2 Тип занятости

# In[23]:


data_jem = data_j.groupby('employment').agg({'id':'count'}).sort_values(by = 'id', ascending = False).reset_index()
data_jem.columns = ['Тип занятости', 'Количество вакансий']
data_jem 


# In[24]:


fig = px.bar(data_jem,
            x = 'Тип занятости',
            y = 'Количество вакансий',
            title = 'Количество вакансий по типу занятости',
            template = 'seaborn')
fig.show()


# ##### Выводы 
# Для людей без опыта работы практически половина вакансий - стажировки. Но непонятно, оплачиваемые они или нет. Другая половина - работа с полной занятостью.

# ##### 3.3.1.3 График работы

# In[25]:


data_js = data_j.groupby('schedule').agg({'id':'count'}).sort_values(by = 'id', ascending = False).reset_index()
data_js.columns = ['График работы', 'Количество вакансий']
data_js


# In[26]:


fig = px.bar(data_js,
            x = 'График работы',
            y = 'Количество вакансий',
            title = 'Количество вакансий по графику работы',
            template = 'seaborn')
fig.show()


# ##### Выводы 
# БОльшая часть вакансий подразумевает работу на полный график. Таких вакансий 19 из 26. 5 вакансий с гибким графиком, 2 - удаленная работа.

# ##### 3.3.1.4 Заработная плата

# К сожалению, дать хоть примерные данные по зарплате невозможно. Слишком много пропусков, которые невозможно заполнить. Посмотрим на таблицу еще раз.

# In[27]:


data_j


# ##### Выводы 
# 
# Из всех 26 строк лишь 2 имеют данные по зп, и те между собой различаются кардинально. Понятно, что почти половина вакансий - стажировки, но и они бывают оплачиваемые. Ответа, к сожалению, нет.

# #### 3.3.2 Junior+

# ##### 3.3.2.1 Топ-работодателей

# In[28]:


data_jje = data_jj.groupby('employer').agg({'id':'count'}).sort_values(by = 'id', ascending = False).reset_index().head(10)
data_jje.columns = ['Работодатель', 'Количество вакансий']
data_jje


# In[29]:


fig = px.bar(data_jje,
            x = 'Работодатель',
            y = 'Количество вакансий',
            title = 'Количество вакансий у различных работадателей',
            template = 'seaborn')
fig.show()


# ##### Выводы
# На этом грейде так же огромное преимущество у Сбера - 60 вакансий. Дальше с большим отрывом идут Ростелеком (10) и ВТБ (8)

# ##### 3.3.2.2 График работы

# In[30]:


data_jjs = data_jj.groupby('schedule').agg({'id':'count'}).sort_values(by = 'id', ascending = False).reset_index()
data_jjs.columns = ['График работы', 'Количество вакансий']
data_jjs


# In[31]:


fig = px.bar(data_jjs,
            x = 'График работы',
            y = 'Количество вакансий',
            title = 'Количество вакансий по графику работы',
            template = 'seaborn')
fig.show()


# ##### Выводы
# БОльшая часть вакансий подразумевает полноценный график (217), но и существенное количество вакансий на удаленную работу (63).

# ##### 3.3.2.3 Тип занятости 

# In[32]:


data_jjem = data_jj.groupby('employment').agg({'id':'count'}).sort_values(by = 'id', ascending = False).reset_index()
data_jjem.columns = ['Тип занятости', 'Количество вакансий']
data_jjem 


# In[33]:


fig = px.bar(data_jjem,
            x = 'Тип занятости',
            y = 'Количество вакансий',
            title = 'Количество вакансий по типу занятости',
            template = 'seaborn')
fig.show()


# ##### Выводы 
# Здесь все очевидно, Джунам+ можно найти работу только на полную занятость.

# ##### 3.3.2.4 Заработная плата

# ##### Выводы 
# К сожалению, ситуация аналогичная, что и со всеми остальными грейдами. По предоставленной таблице мы не можем сделать никаких выводов по заработной плате. 

# #### 3.3.3 Middle

# ##### 3.3.3.1 Топ-работадателей

# In[34]:


data_me = data_m.groupby('employer').agg({'id':'count'}).sort_values(by = 'id', ascending = False).reset_index().head(10)
data_me.columns = ['Работодатель', 'Количество вакансий']
data_me


# In[35]:


fig = px.bar(data_me,
            x = 'Работодатель',
            y = 'Количество вакансий',
            title = 'Количество вакансий у различных работадателей',
            template = 'seaborn')
fig.show()


# ##### Выводы 
# И опять Сбер с огромным отрывом впереди. 61 вакансия против Озона с 13, который расположился на втором месте. Тройку замкнул Газпромбанк с 10 вакансиями. 

# ##### 3.3.3.2 Тип занятости 

# In[36]:


data_mem = data_m.groupby('employment').agg({'id':'count'}).sort_values(by = 'id', ascending = False).reset_index()
data_mem.columns = ['Тип занятости', 'Количество вакансий']
data_mem 


# ##### Выводы 
# Так же, как и у джунов+, только полная занятость. 

# ##### 3.3.3.3 График работы

# In[37]:


data_ms = data_m.groupby('schedule').agg({'id':'count'}).sort_values(by = 'id', ascending = False).reset_index()
data_ms.columns = ['График работы', 'Количество вакансий']
data_ms


# In[38]:


fig = px.bar(data_ms,
            x = 'График работы',
            y = 'Количество вакансий',
            title = 'Количество вакансий по графику работы',
            template = 'seaborn')
fig.show()


# ##### Выводы 
# Удаленной работы относительно общего количества вакансий несколько меньше, чем у джунов+. Доля вакансий с полноценным рабочим днем еще больше, чем у джунов+.

# #### 3.3.4 Senior

# ##### 3.3.4.1 Топ_работадателей 

# In[39]:


data_se = data_s.groupby('employer').agg({'id':'count'}).sort_values(by = 'id', ascending = False).reset_index().head(10)
data_se.columns = ['Работодатель', 'Количество вакансий']
data_se


# In[40]:


fig = px.bar(data_se,
            x = 'Работодатель',
            y = 'Количество вакансий',
            title = 'Количество вакансий у различных работадателей',
            template = 'seaborn')
fig.show()


# ##### Выводы 
# Здесь выборка не очень большая, но впервые Сбер не занимает лидирующего положения. Остальные работадатели встречаются в топах впервые, похоже, что в основном это иностранные компании, где требуются очень скилловые специалисты.

# ##### 3.3.4.2 Тип занятости 

# In[41]:


data_sem = data_s.groupby('employment').agg({'id':'count'}).sort_values(by = 'id', ascending = False).reset_index()
data_sem.columns = ['Тип занятости', 'Количество вакансий']
data_sem 


# ##### Выводы 
# Только полная занятость, других опций у Сеньоров нет. 

# ##### 3.3.4.3 График работы 

# In[42]:


data_ss = data_s.groupby('schedule').agg({'id':'count'}).sort_values(by = 'id', ascending = False).reset_index()
data_ss.columns = ['График работы', 'Количество вакансий']
data_ss


# In[43]:


fig = px.bar(data_ss,
            x = 'График работы',
            y = 'Количество вакансий',
            title = 'Количество вакансий по графику работы',
            template = 'seaborn')
fig.show()


# ##### Выводы 
# Так же, как и у остальных грейдов. Преимущественно работа на полный день.

# ### 3.4 Расчет помесячной динамики

# #### 3.4.1 Junior

# In[44]:


data_jm = data_j.groupby('month').agg({'id':'count'}).sort_values(by = 'id', ascending = False).reset_index()
data_jm


# #### Выводы
# В марте и апреле было оставлено одинаковое количество вакансий на грейд джуна.

# #### 3.4.2 Junior+

# In[45]:


data_jjm = data_jj.groupby('month').agg({'id':'count'}).sort_values(by = 'id', ascending = False).reset_index()
data_jjm


# #### Выводы 
# В апреле на грейд джуна+ было оставлено гораздо больше вакансий, чем в марте, 185 против 104. 

# #### 3.4.3 Middle

# In[46]:


data_mm = data_m.groupby('month').agg({'id':'count'}).sort_values(by = 'id', ascending = False).reset_index()
data_mm


# #### Выводы 
# Ситуация с мидлами пошла дальше: в апреле было оставлено вакансий более чем в два раза больше, чем в марте, 219 против 105.

# #### 3.4.3 Senior

# In[47]:


data_sm = data_s.groupby('month').agg({'id':'count'}).sort_values(by = 'id', ascending = False).reset_index()
data_sm


# #### Выводы 
# У сеньоров количество вакансий, оставленных в апреле практически в три раза превышает те, что были оставлены в марте. И даже нашлась одна вакансия, которая лежит еще с февраля.

# #### Общие выводы
# Мы можем обратить внимание на то, что чем выше грейд, тем больше свежих вакансий относительно предыдущего месяца. Скорее всего, в таблицу не попали вакансии, на которые уже нашлись работники, а остались только актуальные. Можно предположить, что чем ниже требуемые грейд, тем быстрее на него находится работник. Но проверить мы это с этими данными не сможем. 

# # Анализ таблицы с вакансиями на специальность "Аналитик данных"

# ## 1 Получение общей информации

# In[48]:


frame = pd.read_excel("C:\\Users\\prysk\\Downloads\\vacancies_da (2).xlsx")
frame.head(10)


# In[49]:


frame.info()


# ## Выводы
# 
# Ситуция схожая с предыдщуим датафреймом, но строк в два раза меньше. Типы данных в порядке, также очень много пропусков в столбцах с зарплатами, которые не получится заполнить. Также проверим на дубликаты и добавим необходимые для дальнейшей работы столбцы.

# ## 2. Предобработка данных

# ### 2.1 Проверка на дубликаты

# In[50]:


# сначала проверим на явные 
frame.duplicated().sum()


# In[51]:


# и на неявные по id 
frame['id'].duplicated().sum()


# Дубликатов нет. Идем дальше.

# ### 2.2 Добавление новых столбцов

# In[52]:


# сначала заполним заглушками пропуски 
frame['hard_skills_from_description'] = frame['hard_skills_from_description'].fillna(value = '-')
frame['soft_skills_from_description'] = frame['soft_skills_from_description'].fillna(value = '-')
frame['key_skills_from_key_skills_field'] = frame['key_skills_from_key_skills_field'].fillna(value = '-')
# теперь объдиним значения столбцов
frame['key_skills_key'] = frame['hard_skills_from_description'] + ', ' + frame['soft_skills_from_description'] + ', ' + frame['key_skills_from_key_skills_field']


# In[53]:


# посмотрим на уникальные значения нового столбца
frame['key_skills_key'].unique()


# In[54]:


# и приведем самые очевидные к одному значению во избежание повторов
frame['key_skills_key'] = frame['key_skills_key'].str.replace('Data Analysis', 'Анализ данных')


# In[55]:


# добавим столбец с месяцем
frame['month'] = frame['published_at'].dt.to_period("M")


# In[56]:


# посмотрим на то, что у нас получилось 
frame.head(10)


# ### Выводы
# 
# Мы провели предобработку данных. Нами было сделано следующее: 
# - проверили датафрей на дубликаты, коих не обнаружили
# - пропущенные значения в столбах с зароботной платой заполнять не стали по аналогии с предыдущей таблицей
# - также по аналогии добавили столбцы со всеми скиллами и месяцем для удобства дальнейшей работы

# ## 3. Анализ данных

# ### 3.1 Подсчет доли грейдов

# In[57]:


frame_exp = frame.groupby('experience').agg({'id' : 'count'}).sort_values(by = 'id', ascending = False).reset_index()
frame_exp


# In[58]:


# Добавим столбик с процентами и переименуем столбцы
frame_exp['percent'] = frame_exp['id']/frame_exp['id'].sum()*100
frame_exp['percent'] = frame_exp['percent'].round(decimals=1)
frame_exp.columns = ['Требуемый опыт работы', 'Количество вакансий', 'Доля от общего числа']
frame_exp


# In[59]:


# для наглядности построим график 
fig = px.bar(frame_exp,
            x = 'Требуемый опыт работы',
            y = 'Доля от общего числа',
            title = 'Доля вакансий по критерию "Требуемый опыт работы"',
            template = 'seaborn')
fig.show()


# ### Выводы 
# 
# В отличие от специальности "Data Scientist" на аналитика данных доля вакансий на грейд джун+ несколько больше, 57 против 49. И соответственно на мидла меньше, 39 против 48. На сеньоров вакансий всего три, это всего лишь один процент от общего числа. 

# ### 3.2. Определение самых важных скиллов для всех грейдов

# #### 3.2.1 Junior

# In[60]:


# создадим отдельный фрейм по этому грейду
frame_j = frame.query('experience == "Junior (no experince)"')
frame_j['key_skills_key'].unique()


# In[61]:


frame_js = pd.Series(' , '.join(frame_j['key_skills_key']).split(',')).value_counts()[:30]
frame_js


# #### Выводы
# 
# Так как выборка небольшая, то все что мы можем сказать, что из хард-скиллов вам точно потребуется знание SQL и Python.

# #### 3.2.2 Junior+

# In[62]:


frame_jj = frame.query('experience == "Junior+ (1-3 years)"')
frame_jjs = pd.Series(' , '.join(frame_jj['key_skills_key']).lower().split(',')).value_counts()[:30]
frame_jjs


# #### Выводы
# 
# Больше всего аналитикам данных на этом грейде требуется знание SQL (учитывая повтор упоминается 99 раз), Python, в частности знание библиотеки Pandas. 
# 
# Что касается софт-скиллов, то тут на первый план выходит способность к аналитическому мышлению, навыки обращения с документами и навыки коммуникации. 

# #### 3.2.3 Middle

# In[63]:


frame_m = frame.query('experience == "Middle (3-6 years)"')
frame_ms = pd.Series(' , '.join(frame_m['key_skills_key']).lower().split(',')).value_counts()[:30]
frame_ms


# #### Выводы
# Требуемые скиллы для мидла мало чем отличаются от таковых у джуна+. Все та же тройка лидеров среди хард-скиллов. Среди софт-скиллов те же документация и коммуникация, правда способность аналитически мыслить ушла на третье место.

# #### 3.2.4 Senior

# In[64]:


frame_s = frame.query('experience == "Senior (6+ years)"')
frame_ss = pd.Series(' , '.join(frame_s['key_skills_key']).lower().split(',')).value_counts()[:30]
frame_ss


# #### Выводы 
# Так как у нас всего три вакансии на Сеньора, то выборка не очень репрезентативна. В каждой вакансии требуется что-то свое, за исключением знания SQL, который упомянут во всех трех вакансиях. 

# ### 3.3 Определение дополнительных параметров для всех грейдов (Топ-работодателей, зарплата, тип занятости, график работы)

# #### 3.3.1 Junior

# ##### 3.3.1.1 Топ-работодателей

# In[65]:


frame_je = frame_j.groupby('employer').agg({'id':'count'}).sort_values(by = 'id', ascending = False).reset_index()
frame_je.columns = ['Работодатель', 'Количество вакансий']
frame_je


# In[66]:


fig = px.bar(frame_je,
            x = 'Работодатель',
            y = 'Количество вакансий',
            title = 'Количество вакансий у различных работодателей',
            template = 'seaborn')
fig.show()


# ##### Выводы
# В отличие от специалистов в Data Science Сбер не обладает самым большим количеством вакансий, по крайней мере в этом грейде. Больше все вакансий у PECO-Гарантия, 2 штуки. У остальных по одной.

# ##### 3.3.1.2 Тип занятости

# In[67]:


frame_jem = frame_j.groupby('employment').agg({'id':'count'}).sort_values(by = 'id', ascending = False).reset_index()
frame_jem.columns = ['Тип занятости', 'Количество вакансий']
frame_jem 


# ##### Выводы 
# Только полная занятость для этого грейда.
# 

# ##### 3.3.1.3 График работы

# In[68]:


frame_js = frame_j.groupby('schedule').agg({'id':'count'}).sort_values(by = 'id', ascending = False).reset_index()
frame_js.columns = ['График работы', 'Количество вакансий']
frame_js


# ##### Выводы 
# Лишь одна вакансий подразумевает сменный график, остальные восемь полноценный рабочий день.

# #### 3.3.2 Junior+

# ##### 3.3.2.1 Топ_работодателей

# In[69]:


frame_jje = frame_jj.groupby('employer').agg({'id':'count'}).sort_values(by = 'id', ascending = False).reset_index().head(10)
frame_jje.columns = ['Работодатель', 'Количество вакансий']
frame_jje


# In[70]:


fig = px.bar(frame_jje,
            x = 'Работодатель',
            y = 'Количество вакансий',
            title = 'Количество вакансий у различных работодателей',
            template = 'seaborn')
fig.show()


# ##### Выводы 
# У джунов+ главный работодатель Сбер с 42 вакансиями. Все остальные работадатели идут с большим отставанием. 6 у МТС, у остальных 5 и меньше.

# ##### 3.3.2.2 Тип занятости

# In[71]:


frame_jjem = frame_jj.groupby('employment').agg({'id':'count'}).sort_values(by = 'id', ascending = False).reset_index()
frame_jjem.columns = ['Тип занятости', 'Количество вакансий']
frame_jjem 


# ##### Выводы 
# Лишь одна вакансия из 188 на частичную занятость, остальные на полную.

# ##### 3.3.2.3 График работы

# In[72]:


frame_jjs = frame_jj.groupby('schedule').agg({'id':'count'}).sort_values(by = 'id', ascending = False).reset_index()
frame_jjs.columns = ['График работы', 'Количество вакансий']
frame_jjs


# In[73]:


fig = px.bar(frame_jjs,
            x = 'График работы',
            y = 'Количество вакансий',
            title = 'Количество вакансий по графику работы',
            template = 'seaborn')
fig.show()


# ##### Выводы
# БОльша часть вакансий подразумевает работу на полный день. Их 135. 35 вакансий - удаленная работа, 5 - гибкий график, 3 - сменный.

# #### 3.3.3 Middle

# ##### 3.3.3.1 Топ-работодателей

# In[74]:


frame_me = frame_m.groupby('employer').agg({'id':'count'}).sort_values(by = 'id', ascending = False).reset_index().head(10)
frame_me.columns = ['Работодатель', 'Количество вакансий']
frame_me


# In[75]:


fig = px.bar(frame_me,
            x = 'Работодатель',
            y = 'Количество вакансий',
            title = 'Количество вакансий у различных работодателей',
            template = 'seaborn')
fig.show()


# ##### Выводы
# Наибольшое количество вакансий у Сбера - 24, следом идет Вайлдбериз с 11 и «UZUM TECHNOLOGIES» с 5.

# ##### 3.3.3.2 Тип занятости 

# In[76]:


frame_mem = frame_m.groupby('employment').agg({'id':'count'}).sort_values(by = 'id', ascending = False).reset_index()
frame_mem.columns = ['Тип занятости', 'Количество вакансий']
frame_mem 


# ##### Выводы 
# Лишь одна вакансия предусматривает частичную занятость, все остальные 119 полную. 

# ##### 3.3.3.3 График работы

# In[77]:


frame_ms = frame_m.groupby('schedule').agg({'id':'count'}).sort_values(by = 'id', ascending = False).reset_index()
frame_ms.columns = ['График работы', 'Количество вакансий']
frame_ms


# In[78]:


fig = px.bar(frame_ms,
            x = 'График работы',
            y = 'Количество вакансий',
            title = 'Количество вакансий по графику работы',
            template = 'seaborn')
fig.show()


# ##### Выводы 
# Преимущественно, как и во всех остальных грейдах, вакансии подразумевают полный рабочий день. Таких тут 91. Удаленной работы 27, по гибкому графику лишь 2 вакансии.

# #### 3.3.4 Senior

# ##### 3.3.4.1 Топ-работодателей 

# In[79]:


frame_se = frame_s.groupby('employer').agg({'id':'count'}).sort_values(by = 'id', ascending = False).reset_index().head(10)
frame_se.columns = ['Работодатель', 'Количество вакансий']
frame_se


# ##### Выводы 
# Из трех вакансий 2 у Леруа Мерлен. Интересно!

# ##### 3.3.4.2 Тип занятости

# In[80]:


frame_sem = frame_s.groupby('employment').agg({'id':'count'}).sort_values(by = 'id', ascending = False).reset_index()
frame_sem.columns = ['Тип занятости', 'Количество вакансий']
frame_sem 


# ##### Выводы 
# Все вакансии на полную занятость.

# ##### 3.3.4.3 График работы 

# In[81]:


frame_ss = frame_s.groupby('schedule').agg({'id':'count'}).sort_values(by = 'id', ascending = False).reset_index()
frame_ss.columns = ['График работы', 'Количество вакансий']
frame_ss


# ##### Выводы 
# 2 вакансии на полный день, одна на удаленку.

# ### 3.4 Расчет помесячной динамики

# #### 3.4.1 Junior

# In[82]:


frame_jm = frame_j.groupby('month').agg({'id':'count'}).sort_values(by = 'id', ascending = False).reset_index()
frame_jm


# #### Выводы 
# 7 вакансий в апреле против 2 мартовских.

# #### 3.4.2 Junior+

# In[83]:


frame_jjm = frame_jj.groupby('month').agg({'id':'count'}).sort_values(by = 'id', ascending = False).reset_index()
frame_jjm


# #### Выводы 
# В апреле вакансий более чем в два раза больше, чем в марте, 131 против 47

# #### 3.4.3 Middle

# In[84]:


frame_mm = frame_m.groupby('month').agg({'id':'count'}).sort_values(by = 'id', ascending = False).reset_index()
frame_mm


# #### Выводы 
# Та же самая ситуация, что и в грейде ниже, количество вакансий в апреле сильно больше (84), чем в марте (36).

# #### 3.4.4 Senior

# In[85]:


frame_sm = frame_s.groupby('month').agg({'id':'count'}).sort_values(by = 'id', ascending = False).reset_index()
frame_sm


# #### Выводы
# Все три вакансии апрельские.

# # 4. Выводы
# 
# Проект завершен. Подведем итоге. 
# 
# Мы изучили два предложенных датафрейма. Провели предобработку: проверили на дубликаты, добавили новые столбцы и поработали над повторяющимися скиллами, чтобы привести их к более рабочему виду. 
# 
# Провели анализ данных обоих датафреймов, в ходе которых постарались ответить на все поставленные перед нами вопросы:
# 
# - Самую большую долю от всех вакансий на специальность "Data Science" занимает грейд Мидл (около 49%), в то время как на аналитика данных самая большая доля у грейд Джун+ (около 57%). У специалистов Data Science же доля вакансий Джун+ занимает около 44%, а у аналитиков Мидл занимает около 39%. Доля вакансий без опыта очень мала у обоих, но Сеньоров больше требуется в Data Science. Этого говорит нам о том, что в Data Science больше спроса на более квалифицированные кадры, в то время как в аналитике данных с небольшим опытом войти чуть проще. Если у вас вообще нет опыта, то будет непросто и там и там. 
# 
# Теперь отдельно напишем про скиллы, которые требуются на обе специальности по всем грейдам:
# - У просто Джунов без опыта работы выборка вакансий небольшая. Но и аналитиков, и у дата саинтистов чаще всего упомянут Python. На втором месте у аналитиков идет знание SQL, а у дата саинтистов машинное обучение. Что касается софт-скиллов, тот в вакансиях аналитиков часто упомянают аналитическое мышление, а у дата саинтистов документацию. 
# - Если говорить про дата саинтистов в грейде Джун+, то самым важным будет из хард-скиллов знание Python, в частности библиотеки Pandas. На втором месте идет знание SQL и также машинное обучение играет важную роль. У аналитиков то же самое, только SQL уже занимает первое место, а машинное обучение встречается гораздо реже. Среди софт-скиллов и там и там нужно знание документации и способность к комуникации. У аналитиков еще прибавляется аналитическое мышление. 
# - Важность умения в машинное обучение еще больше возрастает у дата саинтистов на грейде Мидл. Оно идет на втором месте сразу после Python, на третьм SQL. У аналитиков же первое и второе место делят Python и SQL. Машинное обучени практически не встречается. Софт-скиллы совпадают с Джуном+ как у аналитиков, так и у датасаинтистов: те же документация и коммуникация. 
# - Выборка у Сеньоров самая маленькая, особенно у аналитиков, где всего три вакансии. У всех требуется знание SQL. По софт-скиллам понимания нет. У дата саинтистов тот же порядок, что и на предыдщем грейде: Python, машинное обучение, SQL. Те же документация и коммуникация в софт-скиллах.
# 
# Дополнительные параметры для всех грейдов:
# - Для джунов среди дата саинтистов основной работодатель - Сбер, у аналитиков на том же грейде как такового лидера нет. У дата саинтистов почти половина вакансий стажировки, в то время как у аналитиков такой опции нет, только полная занятость. Также у саинтистов есть опции и по графику: есть несколько вакансий с удаленной работой и гибким графиком. У аналитиков практически только полный рабочий день.
# - Теперь с джунами+. И у аналитиков, и у дата саинтистов главный работодатель Сбер. Причем с огромным отрывом от остальных. Соотношение удаленной работы к полному дню на этом грейде выровнялось. Тип занятости - только полная, что у аналитиков, что у дата саинтистов. 
# - Мидлы. У дата саинтистов главный работодатель Сбер с очень большим отрывом. У аналитиков тоже Сбер, но довольно много вакансий также у Вайлдбериз. Тип занятости и у саинтистов, и у аналитиков - только полная. Соотношение удаленной работы к полному дню примерно одинаковое. 
# - Сеньоры. У дата саинтистов разброс работодателей большой, как такового лидера нет. У аналитиков всего три вакансии в этом грейде, две из которых от Леруа Мерлен. Что касается тип занятости, то везде полная. График работы у аналитиков: 2 вакансии на полный рабочий день, 1 на удаленную работу. У дата саинтистов 16 на полную, 4 на удаленную.
# 
# Отдельно нужно проговорить про заработную плату. К сожалению, установить ее с нашими данными невозможно. Данные есть лишь у около 8% вакансий в обоих таблицах и рассчитывать и подставлять среднее арифметическое было бы огромным допущением. Скорее всего, работодатель не хочет сразу писать о зароботной плате, чтобы не упускать потенциальных сотрудников. 
# 
# И про динамику: 
# - У дата саинтистов чем выше грейд, тем больше вакансий, оставленных в апреле по отношению к мартовским. Возможно это связано с тем, что те вакансии, на которые был найден работник, не отобразились в таблице. А чем ниже грейд, тем быстрее ищется сотрудник (предположение). Проверить мы это с нашими данными не сможем.
# - У аналитиков такой ситуации нет. Примерно одинаковое отношение апрельских вакансий к мартовским. Возможно, сотрудники ищутся одинаково быстро на всех грейдах. 

# ## Рекомендации 
# 
# Скорее всего вам будет немного проще найти работу, будучи вы аналитиком данных, а не дата саинтистом. Это связано с тем, что доля вакансии на более низкий грейд Джун+ у аналитиков выше, чем у дата саинтистов. 
# 
# С другой стороны, у дата саинтистов есть вакансии со стажировками, чего нет у аналитиков данных. 
# 
# Скиллы, которыми необходимо обладать - примерно схожие, что у аналитиков, что у специалистов в дата сайнс. Кроме того, что дата саинтистам требуется знание машинного обучения, даже на самом низком грейде. Хард-скиллы это почти всегда SQL, Python, в частонсти Pandas, софт-скиллы - документация, коммуникация, у аналитиков аналитическое мышление. Это те скиллы, на которые вам нужно обратить внимание в первую очередь. 
# 
# Рассчитывать на частичную занятость в этих профессиях не приходится, практически везде и на всех грейдах полная. Это нужно иметь ввиду, выбирая это ремесло. 
# 
# Примерно 20% от общего числа вакансии - удаленная работа. Поэтому нужно быть готовым к тому, что на работу придется ездить и соответсвенно выбирать нужно из тех вакансий, которые предлагают компании из вашего города. 

# In[ ]:




